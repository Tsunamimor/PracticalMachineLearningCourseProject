## My Practical Machine Learning Course Project
#By Diana López, Date: "03rd Jan 2019"

## Description
#One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. 
#In this project, your goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants.

##My expectations
Because of the widely used and proven accuracy of Random Forest I expect that this model would be the best with a minimum error, less than 1%
#The training data for this project are available here:

# [Training set](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv)
# [Test set](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv)

#The objective is to correctly predict the variable `classe` of the `Test set`.
#This variable indicates how well the exercise is performed. 
#The value `A` indicates that the exercise was well performed while the other letters (from `B` to `E`) respectively indicate that common mistakes has been done during the execution of the weightlifting.

##The steps to realize the project are:
#1. loading the required packages
#2. loading the data
#3. cleaning the data
#4. building the models
#4.1. classification trees
#4.2. random forest
#4.3. general boosted regression
#5. Validating the best model

## Preparation

### Loading required packages

library(caret)
library(ggplot2)
library(randomForest)
library(rpart)
library(rpart.plot)
library(RColorBrewer)
install.packages("rattle")
install.packages("gmb")
library(rattle)
#library(corrplot)
library(gbm)


### Loading the data


ml_path<-"~/R/Data Science Coursera/"
if(!file.exists(paste(ml_path,"pml-training.csv"))){
  fileUrl <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
  download.file(fileUrl,destfile="./pml-training.csv")
}
if(!file.exists(paste(ml_path,"pml-testing.csv"))){
  fileUrl <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
  download.file(fileUrl,destfile="./pml-testing.csv")
}



training_in <- read.csv("pml-training.csv")
test_in <- read.csv("pml-testing.csv")


### Data cleaning
#All the variables which contain all NA values are discarded.


trainData<- training_in[, colSums(is.na(training_in)) == 0]
validData <- test_in[, colSums(is.na(test_in)) == 0]

#Removing the first seven variables as they have little impact on the outcome classe.

trainData <- trainData[, -c(1:7)]
validData <- validData[, -c(1:7)]

##Modeling

## Preparing the datasets for prediction

#First the `training` dateset is splitted in two datasets:
  
# `trainData`: will be the dataset used to train and test the models
# `validData` : will be the dataset used to validate the models

set.seed(1234) 
inTrain <- createDataPartition(trainData$classe, p = 0.7, list = FALSE)
trainData <- trainData[inTrain, ]
testData <- trainData[-inTrain, ]

#Cleaning even further by removing the variables that are near-zero-variance
NZV <- nearZeroVar(trainData)
trainData <- trainData[, -NZV]
testData  <- testData[, -NZV]

#Models are first trained. Then they are used with the test dataset. 
#Finally a confusion matrix is produced which can be checked to assess the accuracy of the models applied on the validation dataset.

##Classification trees

#Obtaining the model
set.seed(12345)
decisionTreeMod <- rpart(classe ~ ., data=trainData, method="class")

#Using the fancyRpartPlot() function to plot the classification tree as a dendogram.
fancyRpartPlot(decisionTreeMod)

#Testing the model “decisionTreeMod” on the testData to find out how well it performs by looking at the accuracy variable.
predictTreeMod <- predict(decisionTreeMod, testData, type = "class")
cmtree <- confusionMatrix(predictTreeMod, testData$classe)
plot(cmtree$table, col = cmtree$byClass, 
     main = paste("Classification Tree Confusion Matrix: Accuracy =", round(cmtree$overall['Accuracy'], 3)))

##Random forest

#Obtaining the model
set.seed(12345)
controlRF <- trainControl(method="cv", number=3, verboseIter=FALSE)
modRF <- train(classe ~ ., data=trainData, method="rf", trControl=controlRF)
modRF$finalModel
plot(modRF)

#Testing the model obtained “modRF” on the test data to find out how well it performs by looking at the accuracy variable.
predictRF <- predict(modRF, newdata=testData)
cmrf <- confusionMatrix(predictRF, testData$classe)
plot(cmrf$table, col = cmrf$byClass, main = paste("Random Forest Confusion Matrix: Accuracy =", round(cmrf$overall['Accuracy'], 3)))

#Measuring variable importance
rffit <- randomForest(classe ~ ., data=testData, ntree=500, keep.forest=FALSE, importance=TRUE)
rffit$importance # relative importance of predictors (highest <-> most important)
varImpPlot(rffit) # plot results

##Generalized Boosted Regression 

#Obtaining the model
set.seed(12345)
controlGBM <- trainControl(method = "repeatedcv", number = 5, repeats = 1)
modGBM  <- train(classe ~ ., data=trainData, method = "gbm", trControl = controlGBM, verbose = FALSE)
modGBM$finalModel
print(modGBM)

#Testing the model obtained “modGBM” on the test data to find out how well it performs by looking at the accuracy variable.
predictGBM <- predict(modGBM, newdata=testData)
cmGBM <- confusionMatrix(predictGBM, testData$classe)
plot(cmGBM$table, col = cmGBM$byClass, main = paste("GBM Confusion Matrix: Accuracy =", round(cmGBM$overall['Accuracy'], 3)))


#The three models' parameters are:

#decisionTreeMod
#modRF
#modGBM


#The confusion matrix report:

#cmtree
Confusion Matrix and Statistics

          Reference
Prediction   A   B   C   D   E
         A 990 121  15  23   7
         B  32 402  31  56  55
         C  22  94 540 129  78
         D 120 142  74 437 115
         E  12  40  51  34 504

Overall Statistics
                                          
               Accuracy : 0.6967          
                 95% CI : (0.6824, 0.7107)
    No Information Rate : 0.2852          
    P-Value [Acc > NIR] : < 2.2e-16       
                                          
                  Kappa : 0.6174          
 Mcnemar's Test P-Value : < 2.2e-16       

Statistics by Class:

                     Class: A Class: B Class: C Class: D Class: E
Sensitivity            0.8418  0.50313   0.7595   0.6436   0.6640
Specificity            0.9437  0.94767   0.9054   0.8691   0.9593
Pos Pred Value         0.8564  0.69792   0.6257   0.4921   0.7863
Neg Pred Value         0.9373  0.88811   0.9476   0.9252   0.9268
Prevalence             0.2852  0.19374   0.1724   0.1646   0.1840
Detection Rate         0.2401  0.09748   0.1309   0.1060   0.1222
Detection Prevalence   0.2803  0.13967   0.2093   0.2153   0.1554
Balanced Accuracy      0.8928  0.72540   0.8324   0.7563   0.8117

#cmrf

 randomForest(x = x, y = y, mtry = param$mtry) 
               Type of random forest: classification
                     Number of trees: 500
No. of variables tried at each split: 2

        OOB estimate of  error rate: 0.63%
Confusion matrix:
     A    B    C    D    E  class.error
A 3904    2    0    0    0 0.0005120328
B   15 2639    4    0    0 0.0071482318
C    0   15 2380    1    0 0.0066777963
D    0    0   43 2208    1 0.0195381883
E    0    0    1    5 2519 0.0023762376
> cmrf
Confusion Matrix and Statistics

          Reference
Prediction    A    B    C    D    E
         A 1176    0    0    0    0
         B    0  799    0    0    0
         C    0    0  711    0    0
         D    0    0    0  679    0
         E    0    0    0    0  759

Overall Statistics
                                     
               Accuracy : 1          
                 95% CI : (0.9991, 1)
    No Information Rate : 0.2852     
    P-Value [Acc > NIR] : < 2.2e-16  
                                     
                  Kappa : 1          
 Mcnemar's Test P-Value : NA         

Statistics by Class:

                     Class: A Class: B Class: C Class: D Class: E
Sensitivity            1.0000   1.0000   1.0000   1.0000    1.000
Specificity            1.0000   1.0000   1.0000   1.0000    1.000
Pos Pred Value         1.0000   1.0000   1.0000   1.0000    1.000
Neg Pred Value         1.0000   1.0000   1.0000   1.0000    1.000
Prevalence             0.2852   0.1937   0.1724   0.1646    0.184
Detection Rate         0.2852   0.1937   0.1724   0.1646    0.184
Detection Prevalence   0.2852   0.1937   0.1724   0.1646    0.184
Balanced Accuracy      1.0000   1.0000   1.0000   1.0000    1.000
#cmGBM
Confusion Matrix and Statistics

          Reference
Prediction    A    B    C    D    E
         A 1167   23    0    0    1
         B    6  765   14    0    8
         C    2   11  688   19    3
         D    1    0    8  657    9
         E    0    0    1    3  738

Overall Statistics
                                          
               Accuracy : 0.9736          
                 95% CI : (0.9682, 0.9782)
    No Information Rate : 0.2852          
    P-Value [Acc > NIR] : < 2.2e-16       
                                          
                  Kappa : 0.9665          
 Mcnemar's Test P-Value : NA              

Statistics by Class:

                     Class: A Class: B Class: C Class: D Class: E
Sensitivity            0.9923   0.9574   0.9677   0.9676   0.9723
Specificity            0.9919   0.9916   0.9897   0.9948   0.9988
Pos Pred Value         0.9798   0.9647   0.9516   0.9733   0.9946
Neg Pred Value         0.9969   0.9898   0.9932   0.9936   0.9938
Prevalence             0.2852   0.1937   0.1724   0.1646   0.1840
Detection Rate         0.2830   0.1855   0.1668   0.1593   0.1790
Detection Prevalence   0.2888   0.1923   0.1753   0.1637   0.1799
Balanced Accuracy      0.9921   0.9745   0.9787   0.9812   0.9856


## Model selection
#It is possible to see that Random Forest produces the model with the highest accuracy, more than 99%.

##Apply the best model to the validation data
#By comparing the accuracy rate values of the three models,  the ‘Random Forest’ model is the winner. 

final_prediction <- predict(modRF, newdata=validData)
final_prediction

[1] B A B A A E D B A A B C B A E E A B B B
Levels: A B C D E
